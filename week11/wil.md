# Chapter 6. B-Tree Variants
## Copy-on-Write
**Copy-on-Write**는 기존 노드를 직접 수정하지 않고, 복사본을 만들어 작업한 후,<br>
트리의 루트까지 연쇄적으로 복사를 전파한 뒤에 루트 포인터만 한 번에 바꿈.<br>
일관성, 원자성, 장애 복구 및 스냅샷 기능에 유리함.<br>

### **변경 전파 정리**

- **수정이 일어난 경로의 모든 노드** (리프 → 부모 → 조상 → 루트까지)  
  복사되어 새로운 버전을 생성하고, 변경사항을 반영.
- 마지막으로 **루트 포인터** 만 기존 루트에서 새로운 루트(복사본)로 교체.
- 이전 버전의 트리는 그대로 유지되고, **원자적 커밋** 가능.

**장점 요약**
- 트리의 변경 경로를 제외한 나머지는 재사용 가능.
- 트랜잭션 격리와 데이터 무결성 보장.
- 스냅샷, 장애 복구 등에서 높은 안정성.

### LMDB
- **키-값 기반의 초경량 데이터베이스 엔진**
- 메모리 매핑(memory-mapped file) 기법을 사용해 디스크의 데이터 파일을 프로세스 가상 메모리에 직접 연결
- 임베디드 시스템, 머신러닝/딥러닝 데이터셋, 인메모리 캐시 등 다양한 분야에서 널리 활용

## 동작 방식
- **데이터 파일을 메모리에 매핑**
    - 한 번에 파일 전체가 가상 메모리에 맵핑
    - OS가 필요한 데이터만 실제 물리 메모리로 페이지 단위로 불러옴(가상 메모리 관리와 동일)
    - 사용자 입장에서는 모든 데이터가 메모리에 있는 것처럼 접근 가능
- **Copy-on-Write(COW) 구조**
    - 데이터 수정 시, 바뀐 부분과 해당 상위 노드만 새로 복사해 디스크에 기록
    - 전체 파일이 아니라 변경된 블록만 효율적으로 라이트

## 파일/레코드 구조
- **테이블(혹은 데이터베이스 전체)은 보통 하나의 파일로 관리**
- 그 파일 안에 모든 레코드, 인덱스, 메타데이터가 포함됨

## 읽기/쓰기의 효율성
- **읽기:**  별도의 캐시 없이 운영체제의 메모리 매핑만으로 빠른 접근
- **쓰기:**  전체를 다시 쓰는 게 아니라, 수정된 데이터와 그와 연결된 상위 노드만 덮어씀

## 왜 '가볍다'고 하는가?
- 자체 캐시, 로그, 복잡한 관리 구조 없이 단순히 파일-메모리 매핑으로 동작
- 데이터 이동·복제·관리 오버헤드가 적고, 운영체제의 캐시 메커니즘을 그대로 활용
- 변경(쓰기) 시 전체 파일을 다시 쓰지 않고 필요한 조각만 덮어줘서 디스크 IO 효율적

## 대표적인 사용 예시
- **OpenLDAP:** 고성능 디렉터리 서비스에서 LMDB를 기본 데이터베이스 엔진으로 사용


## Abstracting Node Updates
- **B-Tree 노드**는 디스크에 *raw binary* 형태로 저장됨
- 해당 페이지(노드)를 사용할 때 **메모리로 불러와서 가공/수정**하게 됨
- 이때, 디스크에서 불러온 binary 데이터를 **메모리에서 어떻게 표현하고 갱신할지**가 구현 방식에 따라 크게 다름

#### 메모리 내 노드 표현 방식
1. **Raw binary & 포인터/구조체 직접 접근**<br>
binary 데이터를 메모리로 올리고, 포인터와 구조체 캐스팅을 통해 해석 및 수정<br>
메모리 효율 높지만, 동시성·안전성 관리 필요
2. **언어 네이티브 객체/구조체로 변환**<br>
데이터를 언어나 프레임워크 구조체나 객체로 파싱 후 처리<br>
구현 쉽고 안전/메모리 사용량 증가<br>
3. **버퍼 래퍼 객체 사용**<br>
버퍼를 감싸는 객체(wrapper)로 변경시마다 바로 binary에 적용<br>
동시성·버퍼링 유연/관리형 언어에서 주로 사용<br>


## Lazy B-Trees
B-Tree의 update 비용을 줄이고 동시성을 향상시키기 위해 수정사항의 전파를 delay하는 여러 알고리즘이 있음.<br>
### WiredTiger
MongoDB의 storage engine으로, B-Tree 기반의 lazy update를 사용.<br>
B-Tree의 노드를 메모리에 가져오는 것은 동일하지만 메모리에서 관리하는 방식과 수정 사항의 저장 방식이 다름.<br>

메모리에 페이지를 읽어오면 clean 상태로 가져옴.<br>
수정이 일어나면 이 clean page에 직접 반영하는 것이 아니라,<br>
update buffer에 따로 기록함.<br>
만약 이 페이지에 읽으려고 접근하면 clean page와 update buffer를 합쳐서 가장 최신의 내용을 제공함.<br>
이렇게 하면 페이지를 읽는 동안 다른 쓰레드가 업데이트를 할 수 있고, 페이지를 읽는 동안에도 업데이트가 가능함.<br>

이후 수정사항들은 백그라운드의 별도 스레드에서 disk에 기록됨.<br>
만약 수정된 내용을 반영했을 때 페이지의 크기가 최대 크기를 넘어서면 페이지를 분할하여 저장함.<br>

#### SkipList
update buffer에 수정된 내용을 저장할 때 사용하는 구조.<br>
SkipList는 여러 레벨의 링크드 리스트로 구성되어 있어, 빠른 검색과 삽입이 가능함.<br>

### Lazy-Adaptive Tree
**Lazy-Adaptive Tree(LA-Tree)**는 B-Tree 계열의 변형 인덱스 구조로,<br>
많은 양의 삽입/삭제/갱신 같은 업데이트 작업을 효율적으로 처리하기 위한 **배치 처리(batch execution)** 방식이다.<br>
disk에서 B-Tree 대신 LA-Tree를 사용함으로써, 디스크 I/O를 최소화하고 성능을 극대화하는 것을 목표로 한다.<br>

- 디스크 I/O 최적화<br>
  작은 업데이트를 매번 바로 반영하는 대신, 여러 변경 명령을 모아 한 번에 적용함으로써<br>
  디스크 접근 횟수를 획기적으로 줄인다.
- 계층적 버퍼<br>
  Root와 하위 서브트리 각각에 별도의 버퍼가 있어,<br>
  병렬적·동시적 변동 작업이 효율적으로 일어날 수 있다.
- 메모리 효율성<br>
  전체 인덱스를 항상 메모리에 두지 않고, 실제 필요한 노드/버퍼만 불러오는 구조이므로<br>
  대규모 환경에서도 메모리 리소스를 효율적으로 활용한다.

#### 계층적 서브트리와 업데이트 버퍼
- LA-Tree는 **여러 개의 subtree**로 논리적으로 분할된다.
- 각 서브트리마다 **독립적인 update buffer**가 메모리 상에 할당되어 있다.
- 삽입/삭제/수정 등 변경명령은 우선 root buffer 에 *명령 형태*로 임시 저장된다.<br>
  즉, 모든 buffer가 동시에 disk로 flush되는 것이 아니라, 각 서브트리별로 독립적으로 관리된다.

#### 버퍼의 전파와 일괄 반영
- 버퍼가 특정 조건(full 혹은 타이머)에 도달하면, 각 명령(key)에 맞는 위치를 찾아서 하위 서브트리의 buffer로 전파된다.
- 이 과정이 재귀적으로 반복되어, 리프 노드에 도달할 때까지 **연쇄적(cascading)으로 버퍼 플러시**가 발생한다.
- 최종적으로 리프 노드에선 더이상 내려갈 노드가 없으므로 여러 명령이 한 번에 디스크에 반영된다(일괄 처리).

예를 들어 key=42에 대한 삽입 요청이 있었다고 하자.<br>
**[Subtree S0] (Root)**
- 삽입 요청 key=42는 Root 노드의 버퍼에 저장됨<br>
- 디스크에는 즉시 반영되지 않음

**[S0 → S1 전파]**
- Subtree S0의 버퍼가 flush 조건을 만족하면<br>
버퍼의 연산들이 Subtree S1의 루트 (Internal Node A)의 버퍼로 전파됨

**[Subtree S1] (Internal A)**
- 전파된 key=42는 Subtree S1의 버퍼에 저장됨
- 이 역시 당장은 디스크에 반영되지 않음

**[S1 → S2 전파]**
- Subtree S1의 버퍼가 flush 조건을 만족하면,<br>
연산들이 Subtree S2의 루트 (Leaf Node L)의 버퍼로 전파됨

**[Subtree S2] (Leaf L)**
- Leaf 노드 L의 버퍼에 key=42가 저장됨
- Leaf를 루트로 하는 Subtree S2의 버퍼가 flush 조건을 만족하면,<br>
디스크에 key=42가 영구 반영됨

## FD-Trees
database storage에서 많이 쓰는 기법으로 buffering이 있다.<br>
buffering을 하면 random write를 줄일 수 있다.<br>

#### Random Write
radom write란 연속적이지 않은 위치에 데이터를 쓰는 작업을 말한다.<br>
disk의 여러 위치를 왔다갔다하면서 쓰는 작업을 말하는 것이다.<br>

Random write는 굉장히 비효율적이다.<br>
HDD를 쓰는 경우에는 disk의 head가 물리적인 위치를 찾아가는 시간이 필요하기 때문에 느려지고,<br>
SSD를 쓰는 경우에는 위치를 찾는 시간이 필요하지 않지만, 내부에서 garbage collection을 해야 하기 때문에 느려진다.<br>

그런데 B-Tree는 기본적으로 random write를 많이 발생시킨다.<br>
leaf 레벨에 데이터를 삽입하거나 분할/병합과 같은 작업을 할 때 random write가 발생한다.<br>

그래서 FD-Trees에서는 random write를 줄이는 것에 집중하고 그 방법으로 buffering을 사용한다.<br>

#### Random Write를 줄이는 방법
지금까지 살펴본 Lazy B-Trees는 auxiliary buffers(update buffers)를 사용하여 개별/그룹 노드에 대한 버퍼링을 활용했다.<br>
지금부터 살펴볼 Flash Disk Tree에서는 서로 다른 노드들의 변경사항을 append-only 방식으로 한 곳에 모아서 처리한다.<br>

이렇게 하면 target node를 찾아가는 과정이 없이 append만 하면 된다.<br>

#### FD-Tree의 구성
- Head Tree:
    - 메모리에 상주하는 B-Tree
    - 데이터 변경사항을 정렬된 상태로 저장
- immutable sorted runs:
    - 디스크에 저장되는 변경 불가능한 정렬된 데이터의 배열
    - flush된 Head Tree의 데이터를 정렬된 형태로 저장


              ┌──────────────┐
              │  Head Tree   │  ← 메모리 상의 B-Tree
              └─────┬────────┘
                    │
           flush (가득 차면 디스크로 이동)
                    ↓
           ┌────────────────────┐
           │   Level 0 (Runs)   │
           │  ┌──────────────┐  │
           │  │   Run 0      │  │
           │  ├──────────────┤  │
           │  │   Run 1      │  │
           │  ├──────────────┤  │
           │  │   Run 2      │  │
           │  └──────────────┘  │
           └─────────┬──────────┘
                     │ merge (run 초과 시)
                     ↓
           ┌────────────────────┐
           │   Level 1 (Runs)   │
           │  ┌──────────────┐  │
           │  │   Run 0      │  │
           │  ├──────────────┤  │
           │  │   Run 1      │  │
           │  ├──────────────┤  │
           │  │   Run 2      │  │
           │  └──────────────┘  │
           └─────────┬──────────┘
                     │ ...
                     ↓
           ┌────────────────────┐
           │   Level N (Runs)   │
           └────────────────────┘

#### Disk에서의 구조
- level 0~N까지 여러 level로 구성
- 각 level은 정해진 개수만큼의 여러 run으로 계층적 구성
- 각 level은 넘치면 하위 level로 병합
- 탐색 시 위에서 아래로 순차적으로 조회하며 최신값을 선택

### Fractional Cascading
FD-Tree는 여러 level로 되어있기 때문에 탐색 시 각 level을 탐색해야 한다.<br>
다행히 각 level은 정렬되어 있기 때문에 binary search를 할 수 있지만,<br>
깊이가 깊어지면 그만큼 binary search를 해야 하므로 시간이 오래 걸린다.<br>

이를 해결하기 위해서 Fractional Cascading 기법을 사용한다.<br>
간단히 설명하자면 탐색을 위한 지름길을 만드는 것이라고 할 수 있다.<br>

#### 예시
- A1 = [12, 24, 32, 34, 39]
- A2 = [22, 25, 28, 30, 35]
- A3 = [11, 16, 24, 26, 30]

위와 같은 세 개의 정렬된 배열이 있다고 하자.<br>
A1, A2, A3의 순서로 탐색을 한다면 A2의 28을 찾기 위해 binary search를 A1에서 먼저 하고,<br>
그 다음 A2에서 binary search를 해야 한다.<br>

이를 개선하기 위해 A2의 원소 일부를 A1에 추가하여 지름길을 만들어보자.<br>
비슷하게 A3의 원소 일부도 A2에 추가한다.<br>
- A1 = [12, 24, 25, 30, 32, 34, 39]
- A2 = [16, 22, 25, 26, 28, 30, 35]
- A3 = [11, 16, 24, 26, 30]

이렇게 하면 A1에서 28을 찾기 위해 binary search를 할 때,<br>
A1에서 25를 찾고, A2에서 탐색을 시작할 때  25의 위치에서 시작할 수 있다.<br>

### Logarithmic Runs
하위 레벨로 내려갈수록 K배만큼 커진다는 것을 의미한다.<br>
예를 들어 k가 2라고 하면, level 0은 1개의 run, level 1은 2개의 run, level 2는 4개의 run, level 3은 8개의 run이 된다.<br>

#### 삭제는 어떻게 처리하나?
tombstone을 사용한다.<br>
삭제된 데이터임을 표시하는 데이터의 묘비인 셈이다.<br>
이 tombstone은 실제로 데이터를 삭제하는 것이 아니라,<br>
다른 데이터처럼 FD-Tree의 run에 추가된다.<br>
tombstone이 포함된 level이 가득차면 해당 level에 있는 run들의 병합이 일어나고,<br>
병합된 결과는 하위 level로 내려간다.<br>
이 과정이 반복되어 tombstone이 가장 하위의 level에 도달하면,<br>
더 이상 그 데이터가 존재할 하위 level이 없으므로 tombstone은 삭제된다.<br>